{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "pytorch_mnist_solutions.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIbuM-sAxHqv",
        "colab_type": "text"
      },
      "source": [
        "# MNIST data set: recognizing handwritten digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqNXyhd7xHqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "random_seed = 1\n",
        "torch.manual_seed(random_seed);\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzzRE-CoxHqz",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwoaiqILxHq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size_train = 128\n",
        "batch_size_test = 128\n",
        "\n",
        "# training set\n",
        "train_dataset = torchvision.datasets.MNIST('./files/', \n",
        "                train=True, download=True,transform = torchvision.transforms.ToTensor())\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size_train, shuffle=True)\n",
        "# test set\n",
        "test_dataset = torchvision.datasets.MNIST('./files/', train=False, download=True,\n",
        "                             transform = torchvision.transforms.ToTensor())\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size_test, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUzXY0ujxHq2",
        "colab_type": "text"
      },
      "source": [
        "Let us look at some examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPja042nxHq3",
        "colab_type": "code",
        "outputId": "84e1b500-a6a2-40f8-ce70-ba3c9ced8fb9",
        "colab": {}
      },
      "source": [
        "examples = enumerate(train_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "\n",
        "print('Shape of one training mini batch',example_data.shape)\n",
        "print('Shape of one target mini batch',example_targets.shape)\n",
        "print('Example training sample', example_data[1])\n",
        "print('Target values', example_targets[:])\n",
        "print(train_loader.dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of one training mini batch torch.Size([128, 1, 28, 28])\n",
            "Shape of one target mini batch torch.Size([128])\n",
            "Example training sample tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1412,\n",
            "          0.4039, 0.7137, 1.0000, 0.9961, 0.1412, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2941, 0.7686, 0.9412,\n",
            "          0.9922, 0.9922, 0.9922, 0.9922, 0.4588, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0118, 0.2471, 0.7255, 0.9569, 0.9882, 0.9922, 0.9922,\n",
            "          0.9922, 0.9922, 0.9922, 0.9922, 0.2157, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.4980, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7451,\n",
            "          0.4549, 0.4902, 0.9922, 0.9922, 0.5451, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.6196, 0.9608, 0.9490, 0.8078, 0.4863, 0.0863, 0.0549,\n",
            "          0.3725, 0.9412, 0.9922, 0.9686, 0.1569, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.2000, 0.1725, 0.0000, 0.0000, 0.3490, 0.7961,\n",
            "          0.9922, 0.9922, 0.9686, 0.3569, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0275, 0.1451, 0.7922, 0.9922, 0.9922,\n",
            "          0.9922, 0.8510, 0.1490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0157, 0.3490, 0.8000, 0.9922, 0.9922, 0.9922, 0.9922,\n",
            "          0.9922, 0.7647, 0.0275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.7451, 0.9922, 0.9922, 0.9922, 0.9843, 0.5725, 0.5647,\n",
            "          0.7843, 0.9922, 0.6157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.7098, 0.9922, 0.8157, 0.3843, 0.2353, 0.0000, 0.0000,\n",
            "          0.0510, 0.8196, 0.9137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0157, 0.2000, 0.0353, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.7137, 0.9765, 0.2588, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.7137, 0.9804, 0.2706, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.2353, 0.9255, 0.9137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.5804, 0.9922, 0.8627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2353, 0.6392,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3765,\n",
            "          0.9765, 0.9922, 0.5059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1059, 0.8196, 0.8902,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0667, 0.7137, 0.9686,\n",
            "          0.9922, 0.8118, 0.0902, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5490, 0.9922, 0.6157,\n",
            "          0.0000, 0.0000, 0.0000, 0.2314, 0.6706, 0.9294, 0.9922, 0.9922,\n",
            "          0.5255, 0.0941, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5490, 0.9922, 0.7137,\n",
            "          0.4510, 0.4745, 0.8549, 0.9725, 0.9922, 0.9922, 0.9529, 0.3373,\n",
            "          0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5490, 0.9922, 0.9922,\n",
            "          0.9922, 0.9922, 0.9922, 0.9922, 0.8549, 0.6118, 0.0078, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1020, 0.6118, 0.9922,\n",
            "          0.9922, 0.8000, 0.5843, 0.2471, 0.0706, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000]]])\n",
            "Target values tensor([9, 3, 0, 1, 2, 4, 6, 8, 1, 1, 2, 3, 0, 7, 0, 0, 1, 3, 9, 4, 1, 1, 2, 9,\n",
            "        0, 0, 1, 4, 6, 2, 1, 3, 0, 8, 8, 1, 9, 5, 9, 0, 2, 8, 7, 2, 9, 5, 7, 1,\n",
            "        4, 1, 8, 6, 1, 8, 7, 0, 1, 3, 4, 3, 2, 7, 7, 7, 3, 3, 9, 7, 3, 2, 9, 8,\n",
            "        7, 4, 8, 9, 0, 2, 1, 3, 6, 0, 2, 6, 9, 7, 3, 4, 5, 8, 8, 5, 4, 4, 1, 6,\n",
            "        3, 0, 3, 4, 4, 2, 3, 5, 6, 0, 5, 5, 5, 9, 9, 8, 0, 0, 9, 3, 6, 9, 5, 9,\n",
            "        9, 4, 7, 9, 2, 2, 4, 7])\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./files/\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heSntiUvxHq7",
        "colab_type": "text"
      },
      "source": [
        "To get a feeling for the data, we visualize some examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZM32wccxHq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for i in range(6):\n",
        "    plt.subplot(2,3,i+1)\n",
        "    plt.tight_layout()\n",
        "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBVCcZ2AxHq-",
        "colab_type": "text"
      },
      "source": [
        "# Defining the Network\n",
        "\n",
        "Define the network. A few things to keep in mind\n",
        "<ul>\n",
        "    <li> Make sure that the networks dimensions are compatible with our data. The network automatically takes care of handling batches (the first dimension of our data). The remaining dimensions must be met by the data. Thus, we first need to convert the images into vectors before they can be passed through the first linear layer. This can be done in the forward computation of the network.</li>\n",
        "    <li> Each image belongs to one of ten classes. Thus, our output should be 10-dimensional with each output representing one class. To transform the outputs into probabilities, we use can the *softmax* function\n",
        "    \\begin{align}\n",
        "        \\text{softmax}\\left(\\underline{y}\\right) = \\frac{e^{-y_i}}{\\sum_{i}e^{-y_i}}\n",
        "    \\end{align}\n",
        "        that transforms a vector of real numbers into a vector of probabilities. The network can then be trained with the cross entropy loss function. For this to work, we need to transform the labels $y_i\\in\\{0,1\\ldots,9\\}$ into *one-hot* labels \n",
        "    \\begin{align}\n",
        "    y_i = 3 \\quad \\rightarrow \\quad {\\hat{y}}_i = [0,0,0,1,0,0,0,0,0,0]\n",
        "    \\end{align}\n",
        "    </li>\n",
        "<li> **However:** PyTorch has the built-in cost function `nn.CrossEntropyLoss` (see https://pytorch.org/docs/stable/nn.html#crossentropyloss) that takes care of all this. That is, we can just define our network with a linear output layer with ten neurons and then pass the outputs as well as the target values (from 0 to 9) to the loss function. For prediction, we can then simply take the largest value of the outputs are apply the `F.softmax` function if we wish to have probabilities. </li>\n",
        " </ul>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlWSXx2YxHq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    # initialize the network and define all learnable parameters\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.h1  = nn.Linear(28*28,64, bias=True)\n",
        "        self.h2  = nn.Linear(64,64, bias=True)\n",
        "        self.h3  = nn.Linear(64,32, bias=True)\n",
        "        self.h4  = nn.Linear(32,24, bias=True)\n",
        "        self.out = nn.Linear(24,10, bias=True)\n",
        "    # define the forward pass\n",
        "    def forward(self, x):\n",
        "        #x = x.view(-1)\n",
        "        x = torch.flatten(x,start_dim=1) # need to flatten 28x28 image to 784 vector\n",
        "        x = F.relu(self.h1(x)) # First hidden layer\n",
        "        x = F.relu(self.h2(x)) # Second hidden layer\n",
        "        x = F.relu(self.h3(x))\n",
        "        x = F.relu(self.h4(x))\n",
        "        x = self.out(x) # Output layer - no softmax for training since it is included in CrossEntropyLoss!!\n",
        "        #x = torch.nn.Softmax(self.out(x)) # Output layer\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBRx1FNSxHrB",
        "colab_type": "text"
      },
      "source": [
        "Initialize the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3aou9po-xHrB",
        "colab_type": "code",
        "outputId": "f0c3120f-ce09-4ca2-c43f-5890ef144112",
        "colab": {}
      },
      "source": [
        "net_mnist = Net()\n",
        "print(net_mnist)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (h1): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (h2): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (h3): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (h4): Linear(in_features=32, out_features=24, bias=True)\n",
            "  (out): Linear(in_features=24, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wTQCrc3xHrE",
        "colab_type": "text"
      },
      "source": [
        "Define the training procedure as before. To judge the training process, it makes sense to print both the loss values as well as the classification error rates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ada1tG3SxHrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(NeuralNetwork,train_loader,loss_function,num_epochs, learning_rate=0.001, wd=0 ):\n",
        "    \"\"\"\n",
        "    Trains a neural network.\n",
        "    \n",
        "    NeuralNetwork = neural network to be trained\n",
        "    dataloader = DataLoader that deals batches for mini-batch learning\n",
        "    loss_function = cost function to be optimized\n",
        "    num_epochs = number of training epochs\n",
        "    l_rate = learning rate (default value 0.001)\n",
        "    wd = weight decay regularization (default value 0)\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(NeuralNetwork.parameters(), lr = learning_rate, weight_decay=wd)\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        errors = 0\n",
        "        for batch_idx , data in enumerate(train_loader,0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            #print(inputs.shape)\n",
        "            outputs = NeuralNetwork(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            # error rate\n",
        "            predicted = torch.max(outputs,dim=1)\n",
        "            errors += sum(predicted[1] != labels)\n",
        "            #\n",
        "            if (batch_idx % 100) == 0:\n",
        "                #print(batch_idx)\n",
        "                print('Current loss ',running_loss/(batch_idx+1))\n",
        "                #print('Error rate ',errors.numpy())\n",
        "        print('Epoch: ',epoch+1,'Error rate on training set:', round(100.0* errors.numpy() / len(train_loader.dataset),2), '%')\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBvfS4nYxHrM",
        "colab_type": "text"
      },
      "source": [
        "Train the network. Note that since we have a large dataset, we will train fewer epochs than for the small data set used in the previous exercises."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bElunqypxHrN",
        "colab_type": "code",
        "outputId": "aa81b9e3-3799-4690-9b28-50eb03422edd",
        "colab": {}
      },
      "source": [
        "train(net_mnist,train_loader,nn.CrossEntropyLoss(),2,10**-2 )\n",
        "train(net_mnist,train_loader,nn.CrossEntropyLoss(),2,10**-3 )\n",
        "train(net_mnist,train_loader,nn.CrossEntropyLoss(),5,10**-4 )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current loss  2.304649591445923\n",
            "Current loss  0.7189613953970446\n",
            "Current loss  0.5105771936142622\n",
            "Current loss  0.42165253130700503\n",
            "Current loss  0.37492396288604807\n",
            "Epoch:  1 Error rate on training set: 10.77 %\n",
            "Current loss  0.24892480671405792\n",
            "Current loss  0.1738187460468547\n",
            "Current loss  0.17345916500213135\n",
            "Current loss  0.16835951015501718\n",
            "Current loss  0.1673343702686249\n",
            "Epoch:  2 Error rate on training set: 4.81 %\n",
            "Current loss  0.1304633617401123\n",
            "Current loss  0.09535175178310659\n",
            "Current loss  0.09047527539900582\n",
            "Current loss  0.08939188388037028\n",
            "Current loss  0.08834423536337224\n",
            "Epoch:  1 Error rate on training set: 2.51 %\n",
            "Current loss  0.08207610994577408\n",
            "Current loss  0.0668713088599172\n",
            "Current loss  0.0686448635626121\n",
            "Current loss  0.0685886718577201\n",
            "Current loss  0.071328482489587\n",
            "Epoch:  2 Error rate on training set: 2.08 %\n",
            "Current loss  0.06343777477741241\n",
            "Current loss  0.06065725842214162\n",
            "Current loss  0.057312307188945324\n",
            "Current loss  0.05675134513267251\n",
            "Current loss  0.05583683078170306\n",
            "Epoch:  1 Error rate on training set: 1.63 %\n",
            "Current loss  0.023882664740085602\n",
            "Current loss  0.052255570450521045\n",
            "Current loss  0.05505460947265487\n",
            "Current loss  0.05609699009385492\n",
            "Current loss  0.05483523814037832\n",
            "Epoch:  2 Error rate on training set: 1.57 %\n",
            "Current loss  0.053475115448236465\n",
            "Current loss  0.05448699946801114\n",
            "Current loss  0.05439448969864949\n",
            "Current loss  0.05167518831183051\n",
            "Current loss  0.05315548493213339\n",
            "Epoch:  3 Error rate on training set: 1.51 %\n",
            "Current loss  0.016389157623052597\n",
            "Current loss  0.050083821261356964\n",
            "Current loss  0.05173416058556061\n",
            "Current loss  0.05172071538005408\n",
            "Current loss  0.05232375545399333\n",
            "Epoch:  4 Error rate on training set: 1.48 %\n",
            "Current loss  0.045709624886512756\n",
            "Current loss  0.04831145459435659\n",
            "Current loss  0.05090084299445152\n",
            "Current loss  0.05061042185971804\n",
            "Current loss  0.05064154161312188\n",
            "Epoch:  5 Error rate on training set: 1.45 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMeF289sxHrP",
        "colab_type": "text"
      },
      "source": [
        "In order to evaluate our model properly and avoid overfitting, we need to run the network on the training set.\n",
        "Write a routine that computes the classification error rate on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMXZmu0HxHrP",
        "colab_type": "code",
        "outputId": "16c82ded-aa83-4fb6-9176-7209b343d8c6",
        "colab": {}
      },
      "source": [
        "errors_test = 0\n",
        "for batch_idx , data in enumerate(test_loader,0):\n",
        "    inputs, labels = data\n",
        "    outputs = net_mnist(inputs)\n",
        "    # error rate\n",
        "    predicted = torch.max(outputs,dim=1)\n",
        "    errors_test += sum(predicted[1] != labels)\n",
        "print('Error rate on test set:', round(100.0* errors_test.numpy() / len(test_loader.dataset),2), '%')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error rate on test set: 2.88 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXAdPKI_xHrS",
        "colab_type": "text"
      },
      "source": [
        "Suggestion for further work: \n",
        "<ul>\n",
        "    <li> If your error rates on the training and test set differ significantly, your model is overfitting. What can you do against this? </li>\n",
        "<li> If you achieve a low error rate on the test set: find the images that are classified incorrectly by the network. Would you classify those correctly? </li>\n",
        "<li> For comparison of your networks performance, you can take a look at the Wikipedia page: https://en.wikipedia.org/wiki/MNIST_database </li>\n",
        "</ul>\n"
      ]
    }
  ]
}